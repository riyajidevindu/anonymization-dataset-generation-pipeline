# Anonymization Dataset Pipeline

Generate realistic Employment, Education & Social-Context prompts, anonymize sensitive ones, and stream everything straight into Google Sheets. This project also includes a pipeline to extract PII values from the generated dataset.

---

## ğŸ“ Folder Structure

```
anonymization-pipeline/
â”‚
â”œâ”€ app/                      â† all Python code
â”‚   â”œâ”€ __init__.py
â”‚   â”œâ”€ config.py             â† loads secrets from .env
â”‚   â”œâ”€ prompt_definitions.py â† system prompts (Yes / No)
â”‚   â”œâ”€ gemini_client.py      â† calls Gemini API
â”‚   â”œâ”€ sheets_client.py      â† Google Sheets helper
â”‚   â”œâ”€ generate_dataset.py   â† rotates prompts, pushes rows
â”‚   â”œâ”€ extract_pii_values.py â† extracts PII values from the dataset
â”‚   â””â”€ state.json            â† remembers last prompt mode
â”‚
â”œâ”€ credentials/              â† **keep out of Git!**
â”‚   â””â”€ service_account.json  â† Google service-account key
â”‚
â”œâ”€ .env                      â† real secrets (not committed)
â”œâ”€ .env.example              â† template with dummy values
â”œâ”€ .gitignore
â”œâ”€ requirements.txt
â”œâ”€ run.py                    â† entry point: `python run.py`
â””â”€ README.md                 â† you are here
```

---

## âœ¨ What the Pipeline Does

### Dataset Generation
1. **Rotates prompts**: first run â†’ PII-heavy (*Yes*), second run â†’ safe / tricky (*No*), then repeats.
2. **Calls Gemini-Pro** (or Flash) to generate `settings.NUM_ROWS` dataset rows.
3. **Cleans & parses** the JSON returned by Gemini.
4. **Appends** new rows to the chosen Google Sheet/tab.
5. **Logs** the API response and remembers which prompt type was last used.

### PII Value Extraction
1.  **Processes each sheet tab in parallel.**
2.  **Identifies rows** where "Need Anonymization" is "yes" and "PII Value" is empty.
3.  **Calls the Gemini API** to extract PII values from the "Orginal" text.
4.  **Cleans the API response** to ensure only comma-separated values are stored.
5.  **Updates the "PII Value" column** for each row.
6.  **Allows graceful stopping** with Ctrl+C.

---

## ğŸ”§ Prerequisites

| Requirement               | Notes                                                                                     |
| ------------------------- | ----------------------------------------------------------------------------------------- |
| **Python 3.9+**           | Any modern CPython works                                                                  |
| Google **Gemini API key** | Create in [Google AI Studio](https://aistudio.google.com/app/apikey)                      |
| **Google Cloud project**  | Enable **Google Sheets API**                                                              |
| **Service Account JSON**  | Give the service account *Editor* access to your target Sheet                             |
| **Google Sheet**          | Create a sheet; add a tab named `Employment, Education & Social Context`; row 1 = headers |

---

## ğŸš€ Setup Steps

1. **Clone the repository**

   ```bash
   git clone https://github.com/<you>/anonymization-pipeline.git
   cd anonymization-pipeline
   ```

2. **Create a virtual environment**

   ```bash
   python -m venv venv
   # macOS/Linux
   source venv/bin/activate
   # Windows
   venv\Scripts\activate
   ```

3. **Install dependencies**

   ```bash
   pip install --upgrade pip
   pip install -r requirements.txt
   ```

4. **Add secrets**

   ```bash
   cp .env.example .env        # then open .env in an editor
   mkdir -p credentials
   mv <downloaded-key>.json credentials/service_account.json
   ```

   Edit `.env` and fill in:

   ```ini
   GEMINI_API_KEY=AIzaSy...
   GOOGLE_SHEET_ID=1AbCdefGhij...        # spreadsheet ID from the URL
   SERVICE_ACCOUNT_FILE=credentials/service_account.json
   NUM_ROWS=10                           # rows generated per run
   SHEET_TAB_NAME=Employment, Education & Social Context
   SHEET_TABS=Sheet1,Sheet2,Sheet3,Sheet4 # tabs for PII extraction
   ```

5. **Share the Sheet with your service account**

   - Open the Sheet â†’ **Share** â†’ add `your-service@project.iam.gserviceaccount.com` â†’ **Editor**.

6. **Initial test**

   ```bash
   # Generate dataset
   python run.py generate

   # Extract PII values
   python run.py extract_pii
   ```

   You should see `ğŸ“¤ Writing 10 rows...` and a successful API response for generation, and the PII extraction process should start.

---

## ğŸƒ Running Regularly

### Dataset Generation (Linux / macOS - cron)

```
# run every hour at minute 0
0 * * * * /full/path/venv/bin/python /full/path/anonymization-pipeline/run.py generate >> /full/path/pipeline.log 2>&1
```

### Dataset Generation (Windows Task Scheduler)

1. Create Basic Task â†’ Trigger: Daily / Hourly.
2. Action: *Start a Program* â†’ `python.exe`.
3. Arguments: `C:\...\anonymization-pipeline\run.py generate`.
4. Start in: `C:\...\anonymization-pipeline`.

### PII Value Extraction

Run this command manually when you need to update the "PII Value" column:
```bash
python run.py extract_pii
```
You can stop the process at any time by pressing **Ctrl+C**.

---

## ğŸ”„ Changing Prompt Behaviour

- **Rows per run** â†’ edit `NUM_ROWS` in `.env`.
- **Prompt content** â†’ edit `app/prompt_definitions.py`.
- **Rotation order** â†’ change `PROMPT_SEQUENCE` list.

---

## ğŸ§° Troubleshooting

| Symptom            | Fix                                                                                       |
| ------------------ | ----------------------------------------------------------------------------------------- |
| `JSON Parse Error` | Gemini wrapped output in markdown - cleanup handled in code. Check `clean_gemini_json()`. |
| Rows not written   | Ensure service account has *Editor* rights and `SHEET_TAB_NAME` matches tab exactly.      |
| Quota errors       | Check AI Studio quota dashboard or upgrade plan.                                          |
| Duplicate rows     | Use timestamp column (already included) or add logic to skip if identical.                |
| Dirty PII values   | The `clean_pii_values` function in `extract_pii_values.py` should handle this. You can improve the cleaning logic there if needed. |
